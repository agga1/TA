{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agnieszka Dutka\n",
    "# Laboratory 3 - Huffman algorithm\n",
    "\n",
    "Contents:  \n",
    "[Classic Huffman](#classic)  \n",
    "[Adaptive Huffman](#adaptive)  \n",
    "[Summary](#evaluation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitarray import bitarray\n",
    "from collections import defaultdict\n",
    "files_dir = \"files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classic'></a>\n",
    "## Classic Huffman algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, letter, weight, children=[]):\n",
    "        self.letter =letter\n",
    "        self.weight = weight\n",
    "        self.children = children\n",
    "        \n",
    "    def to_dict(self, dic, prefix=\"\"):\n",
    "        if self.letter is not None:\n",
    "            dic[self.letter] = bitarray(prefix)\n",
    "        for idx, child in enumerate(self.children):\n",
    "            child.to_dict(dic, prefix+str(idx))\n",
    "            \n",
    "    def __setitem__(self, index, value):\n",
    "        self.children[index] = value\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.children[index]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.letter}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Huffman:\n",
    "    \"\"\" class containing huffman encoding in form of dictionary and tree (for quick coding and encoding), \n",
    "        based on provided text. \n",
    "        Class provides methods to compress and decompress files based on this encoding, and create encoding itself.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        f = open(files_dir+file, \"r\", encoding='utf-8')\n",
    "        text = ''.join(f.readlines())\n",
    "        letter_freq = Huffman.get_letter_freq(text)  # creating dictionary\n",
    "        self.root = Huffman.huffman(letter_freq)  # creating tree\n",
    "        self.dic = {}\n",
    "        self.root.to_dict(self.dic)  # creating dictionay with letter codes\n",
    "        self.file = file  # only name saved, for documentation purposes only\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_letter_freq(text):  # creates dictionary\n",
    "        letter_freq = {}\n",
    "        for c in text:\n",
    "            letter_freq[c] = 1 if c not in letter_freq.keys() else letter_freq[c]+1\n",
    "        return letter_freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def huffman(letter_counts: dict):\n",
    "        nodes=[]\n",
    "        for a,weight in letter_counts.items():\n",
    "            nodes.append(Node(a,weight))\n",
    "        internal_nodes=[]\n",
    "        leafs=sorted(nodes,key=lambda n:n.weight)\n",
    "        while(len(leafs)+len(internal_nodes)>1):\n",
    "            head=[]\n",
    "            if(len(leafs)>=2):\n",
    "                head+=leafs[:2]\n",
    "            elif(len(leafs)==1):\n",
    "                head+=leafs[:1]\n",
    "            if(len(internal_nodes)>=2):\n",
    "                head+=internal_nodes[:2]\n",
    "            elif(len(internal_nodes)==1):\n",
    "                head+=internal_nodes[:1]\n",
    "            element_1,element_2=sorted(head,key=lambda n:n.weight)[:2]\n",
    "            internal_nodes.append(Node(None, element_1.weight+element_2.weight, [element_1, element_2]))\n",
    "            if(len(leafs)>0 and element_1==leafs[0]):\n",
    "                leafs=leafs[1:]\n",
    "            else:\n",
    "                internal_nodes=internal_nodes[1:]\n",
    "            if(len(leafs)>0and element_2==leafs[0]):\n",
    "                leafs=leafs[1:]\n",
    "            else:internal_nodes=internal_nodes[1:]\n",
    "        return internal_nodes[0]\n",
    "    \n",
    "    def compress(self, to_file=None):\n",
    "        f = open(files_dir+self.file, \"r\", encoding='utf-8')\n",
    "        text = ''.join(f.readlines())\n",
    "        f.close()\n",
    "        compressed = bitarray()\n",
    "        for c in text:\n",
    "            compressed += self.dic[c]\n",
    "        if to_file is not None:\n",
    "            compressed_f = open(to_file, \"wb\")\n",
    "            extra_int = (8-len(compressed)%8)%8\n",
    "            extra_b = bitarray()  # save info in first byte how many extra bytes at the end (to fill to full byte)\n",
    "            extra_b.frombytes(str(extra_int).encode('utf-8'))\n",
    "            compressed_f.write(extra_b)\n",
    "            compressed_f.write(compressed.tobytes())\n",
    "            compressed_f.close()\n",
    "        return compressed\n",
    "    \n",
    "    def decompress(self, file, to_file=None):\n",
    "        array = bitarray()\n",
    "        with open(file, \"rb\") as f:\n",
    "            array.frombytes(f.read())\n",
    "            f.close()\n",
    "        extra = int(array[:8].tobytes().decode()) # delete reduntant bits filling to full bytes\n",
    "        array = array[8:-extra]\n",
    "        decompressed = \"\"\n",
    "        i = 0\n",
    "        node = self.root\n",
    "        while i < len(array):\n",
    "            while node.letter is None:\n",
    "                node = node[array[i]]\n",
    "                i += 1\n",
    "            decompressed += node.letter\n",
    "            node = self.root\n",
    "        if to_file is not None:\n",
    "            with open(to_file, \"w\", encoding='utf-8') as f:\n",
    "                f.write(decompressed)\n",
    "                f.close()\n",
    "        return decompressed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "compressing file to \"compressed\" folder, and recompressing to \"decompressed.txt\"\n",
    "Content of decompressed file is then printed, to check if text was correctly decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User pages are administration pages in the User and User talk namespaces that are useful for organizing and aiding the work users do on Wikipedia, as well as facilitating interaction and sharing between users. User pages are mainly for interpersonal discussion, notices, testing and drafts (see: Sandboxes), and, if desired, limited autobiographical and personal content.\n",
      "\n",
      "User pages are available to Wikipedia users personally for purposes compatible with the Wikipedia project and acceptable to the community; Wikipedia is not a blog, webspace provider, or social networking site. Wikipedia policies concerning the content of pages can and generally do apply to user pages, and users must observe these policies. Users believed to be in violation of these policies should first be advised on their talk page using {{subst:uw-userpage}} when immediate action is not otherwise necessary. \n",
      "Your in this context means associated with you, not belonging to you. \n",
      "something else yet to be seen.\n"
     ]
    }
   ],
   "source": [
    "to_file = \"compressed\"\n",
    "huff = Huffman(\"1KB.txt\")\n",
    "huff.compress(to_file)\n",
    "huff.decompress(to_file, \"decompressed.txt\")\n",
    "with open(\"decompressed.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adaptive'></a>\n",
    "## Adaptive huffman algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdNode:\n",
    "    def __init__(self, letter=\"##\", parent=None):\n",
    "        self.weight = 0\n",
    "        self.letter = letter\n",
    "        self.parent = parent\n",
    "        self.children = [None, None]\n",
    "\n",
    "    def get_code(self, curr_code):\n",
    "        if self.parent:\n",
    "            code = int(not self.parent[0] == self)\n",
    "            return self.parent.get_code(curr_code + str(code))\n",
    "        else:\n",
    "            return curr_code\n",
    "\n",
    "    def add_children(self, left, right):  # both weight 0 at th beginning\n",
    "        self[0] = left\n",
    "        self[1] = right\n",
    "\n",
    "    @staticmethod\n",
    "    def swap(node1, node2):\n",
    "        is_left = not node1.parent[0] == node1\n",
    "\n",
    "        if node2.parent[0] == node2:\n",
    "            node2.parent[0] = node1\n",
    "        else:\n",
    "            node2.parent[1] = node1\n",
    "\n",
    "        node1.parent, node2.parent = node2.parent, node1.parent\n",
    "        node2.parent[is_left] = node2\n",
    "\n",
    "    def increment(self):\n",
    "        self.weight += 1\n",
    "        if self.uncle and self.uncle.weight < self.weight:\n",
    "            AdNode.swap(self, self.uncle)\n",
    "\n",
    "        if self.parent:\n",
    "            if self.parent[0] == self and self.parent[1].weight < self.weight:\n",
    "                AdNode.swap(self, self.parent[1])\n",
    "            self.parent.increment()\n",
    "\n",
    "    @property\n",
    "    def code(self):\n",
    "        code = self.get_code(bitarray())\n",
    "        code.reverse()\n",
    "        return code\n",
    "\n",
    "    @property\n",
    "    def uncle(self):\n",
    "        if self.parent and self.parent.parent:\n",
    "            if self.parent == self.parent.parent[0]:\n",
    "                return self.parent.parent[1]\n",
    "            else:\n",
    "                return self.parent.parent[0]\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.children[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.children[key] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Huffman structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveHuffman:\n",
    "    def __init__(self, file):\n",
    "        self.file = file  # only name saved, for documentation purposes only\n",
    "\n",
    "    def compress(self, to_file=None):\n",
    "        f = open(files_dir + self.file, \"r\", encoding='utf-8')\n",
    "        text = ''.join(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "        nodes = {\"##\": AdNode()}\n",
    "        compressed = bitarray()\n",
    "\n",
    "        for letter in list(text):\n",
    "            if letter in nodes:\n",
    "                node = nodes[letter]\n",
    "                compressed += node.code\n",
    "                node.increment()\n",
    "            else:\n",
    "                updated_node = nodes[\"##\"]\n",
    "                compressed += updated_node.code\n",
    "                \n",
    "                letter_b = bitarray()\n",
    "                letter_b.frombytes(bytes(letter, \"utf-8\"))\n",
    "                if (len(letter_b) > 8):\n",
    "                    print(\"letter occupies\", len(letter_b) / 8, \"bytes!\")\n",
    "                    return -1\n",
    "                compressed += letter_b\n",
    "\n",
    "                node = AdNode(letter=letter, parent=updated_node)\n",
    "                nyt_node = AdNode(parent=updated_node)\n",
    "                updated_node.add_children(nyt_node, node)\n",
    "                del nodes[\"##\"]\n",
    "                nodes[\"##\"] = nyt_node\n",
    "                nodes[letter] = node\n",
    "                \n",
    "                node.weight +=1\n",
    "                updated_node.increment()\n",
    "\n",
    "        if to_file is not None:\n",
    "            compressed_f = open(to_file, \"wb\")\n",
    "            extra_int = (8 - len(compressed) % 8) % 8\n",
    "            extra_b = bitarray()  # save info in first byte how many extra bytes at the end (to fill to full byte)\n",
    "            extra_b.frombytes(str(extra_int).encode('utf-8'))\n",
    "            compressed_f.write(extra_b)\n",
    "            compressed_f.write(compressed.tobytes())\n",
    "            compressed_f.close()\n",
    "\n",
    "        return compressed\n",
    "\n",
    "    def decompress(self, file, to_file=None):\n",
    "        # get compressed text\n",
    "        compressed = bitarray()\n",
    "        with open(file, \"rb\") as f:\n",
    "            compressed.frombytes(f.read())\n",
    "            f.close()\n",
    "\n",
    "        # delete reduntant bits filling to full bytes\n",
    "        extra = int(compressed[:8].tobytes().decode())\n",
    "        if extra == 0:\n",
    "            compressed = compressed[8:]\n",
    "        else:\n",
    "            compressed = compressed[8:-extra]\n",
    "\n",
    "        nodes = {\"##\": AdNode()}\n",
    "        root = nodes[\"##\"]\n",
    "        text = \"\"\n",
    "        idx = 0\n",
    "\n",
    "        while idx < len(compressed):\n",
    "            curr_node = root\n",
    "\n",
    "            while curr_node.weight > 0 and curr_node.letter == \"##\":\n",
    "                curr_node = curr_node[compressed[idx]]\n",
    "                idx += 1\n",
    "\n",
    "            if curr_node.letter != \"##\":\n",
    "                letter = curr_node.letter\n",
    "\n",
    "                node = nodes[letter]\n",
    "                node.increment()\n",
    "\n",
    "            else:\n",
    "                # decompress from 1 byte\n",
    "                letter = compressed[idx:(idx + 8)].tobytes().decode(\"utf-8\")\n",
    "                idx += 8\n",
    "                \n",
    "                # fork current nyt_node\n",
    "                updated_node = nodes[\"##\"]\n",
    "                node = AdNode(letter=letter, parent=updated_node)\n",
    "                nyt_node = AdNode(parent=updated_node)\n",
    "                updated_node.add_children(nyt_node, node)\n",
    "                \n",
    "                # update dictionary\n",
    "                del nodes[\"##\"]\n",
    "                nodes[\"##\"] = nyt_node\n",
    "                nodes[letter] = node\n",
    "\n",
    "                node.weight+=1\n",
    "                updated_node.increment()\n",
    "            text += letter\n",
    "            \n",
    "        if to_file is not None:\n",
    "            with open(to_file, \"w\", encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "                f.close()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "compressing file to \"compressed\" folder, and recompressing to \"decompressed.txt\"\n",
    "Content of decompressed file is then printed, to check if text was correctly decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non dapibus dolor, non sodales felis. Curabitur vel tellus magna. Phasellus mattis sem non libero auctor ultrices. Aenean pellentesque semper ultricies. Vivamus est justo, congue vel pharetra vitae, sagittis a augue. Vivamus vitae arcu odio. Duis commodo, turpis sed vehicula tempor, lorem arcu blandit lectus, in gravida mi risus nec orci. Nullam et suscipit massa.\n",
      "\n",
      "Pellentesque posuere porta felis, at accumsan orci fermentum non. Donec metus leo, fermentum efficitur consectetur a, ultricies a eros. Maecenas vulputate sem eget erat bibendum, vitae molestie eros ultricies. Fusce ut egestas mauris. Suspendisse posuere commodo justo, imperdiet consectetur nulla ornare nec. Cras porta leo vel tortor tempus, nec feugiat mi ultricies. Sed pretium tempor pretium. Donec magna sem, convallis nec efficitur eu, blandit in ante.\n",
      "\n",
      "Curabitur at dignissim erat, eget condimentum ipsum. Vivamus at enim congue, finibus sapien quis, aliquet posuere. \n"
     ]
    }
   ],
   "source": [
    "to_file = \"compressed\"\n",
    "huff = AdaptiveHuffman(\"1KB.txt\")\n",
    "huff.compress(to_file)\n",
    "huff.decompress(to_file, \"decompressed.txt\")\n",
    "with open(\"decompressed.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## Evaluation\n",
    "For given files of size: 1kB, 10kB, 100kB and 1MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import os\n",
    "def test_compression(file, Class):\n",
    "    \"\"\" test provided coding class (must implement compress and decompress methods) \"\"\"\n",
    "    compressed = \"compressed\"\n",
    "    tstart = perf_counter()\n",
    "    huff = Class(file)\n",
    "    tend1 = perf_counter()\n",
    "    huff.compress(compressed)\n",
    "    tend2 = perf_counter()\n",
    "    huff.decompress(compressed)\n",
    "    tend3 = perf_counter()\n",
    "    \n",
    "    print(f\"Time evaluation for {file}:\\nstructure creation:{tend1-tstart}\\\n",
    "            \\ncompression: {tend2-tend1}\\ndecompression: {tend3-tend2}\")\n",
    "    \n",
    "    original_size = os.stat(files_dir+file).st_size\n",
    "    compressed_size = os.stat(compressed).st_size\n",
    "    ratio = 1-compressed_size/original_size\n",
    "    print(f\"original size: {original_size/1000}kB\")\n",
    "    print(f\"compresion ratio: {round(ratio*100, 2)}%\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time evaluation for 1KB.txt:\n",
      "structure creation:0.0012965000005351612            \n",
      "compression: 0.0006821999995736405\n",
      "decompression: 0.00228889999925741\n",
      "original size: 1.005kB\n",
      "compresion ratio: 46.57%\n",
      "\n",
      "Time evaluation for 10KB.txt:\n",
      "structure creation:0.003115200001047924            \n",
      "compression: 0.0032797000003483845\n",
      "decompression: 0.020794299998669885\n",
      "original size: 10.1kB\n",
      "compresion ratio: 46.43%\n",
      "\n",
      "Time evaluation for 100KB.txt:\n",
      "structure creation:0.019422600000325474            \n",
      "compression: 0.014856799998597126\n",
      "decompression: 0.1564118000005692\n",
      "original size: 100.657kB\n",
      "compresion ratio: 46.64%\n",
      "\n",
      "Time evaluation for 1MB.txt:\n",
      "structure creation:0.19242510000003676            \n",
      "compression: 0.15088209999885294\n",
      "decompression: 1.7945044000007329\n",
      "original size: 1038.587kB\n",
      "compresion ratio: 47.29%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_compression(\"1KB.txt\", Huffman)\n",
    "test_compression(\"10KB.txt\", Huffman)\n",
    "test_compression(\"100KB.txt\", Huffman)\n",
    "test_compression(\"1MB.txt\", Huffman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time evaluation for 1KB.txt:\n",
      "structure creation:1.4000015653437003e-06            \n",
      "compression: 0.018384399998467416\n",
      "decompression: 0.012035200001264457\n",
      "original size: 1.005kB\n",
      "compresion ratio: 40.3%\n",
      "\n",
      "Time evaluation for 10KB.txt:\n",
      "structure creation:1.0999992809956893e-06            \n",
      "compression: 0.11195910000060394\n",
      "decompression: 0.08279489999949874\n",
      "original size: 10.1kB\n",
      "compresion ratio: 43.69%\n",
      "\n",
      "Time evaluation for 100KB.txt:\n",
      "structure creation:1.2000000424450263e-06            \n",
      "compression: 1.1390696999987995\n",
      "decompression: 0.9539350000013656\n",
      "original size: 100.657kB\n",
      "compresion ratio: 44.31%\n",
      "\n",
      "Time evaluation for 1MB.txt:\n",
      "structure creation:1.3999997463542968e-06            \n",
      "compression: 11.683703900000182\n",
      "decompression: 9.329767499999434\n",
      "original size: 1038.587kB\n",
      "compresion ratio: 44.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_compression(\"1KB.txt\", AdaptiveHuffman)\n",
    "test_compression(\"10KB.txt\", AdaptiveHuffman)\n",
    "test_compression(\"100KB.txt\", AdaptiveHuffman)\n",
    "test_compression(\"1MB.txt\", AdaptiveHuffman)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
